{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anupamav/NVS-GAN/blob/main/NVS_GAN_V1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2IzfR-ruKyXg",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow-addons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KyRwH9YlLqIE"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import pathlib\n",
        "from PIL import Image\n",
        "import random\n",
        "import tensorflow_addons as tfa\n",
        "import csv\n",
        "\n",
        "from tensorflow.keras.layers import Input, DepthwiseConv2D, Conv2D,Concatenate, Activation,Conv2DTranspose,Flatten, add,  concatenate\n",
        "from tensorflow.keras.layers import Reshape, Conv2DTranspose, BatchNormalization, UpSampling2D, Add, Layer, SeparableConv2D\n",
        "from tensorflow.keras.layers import Dense, Input, ReLU, Lambda, LeakyReLU, ELU\n",
        "from tensorflow.keras import datasets, layers\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.losses import MeanSquaredError\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from IPython import display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "avm7hHXcL-Ff"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NbFhKsNJL-2o"
      },
      "outputs": [],
      "source": [
        "data = np.load('/content/drive/MyDrive/train_car_256.npy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dd63alCOMFPP"
      },
      "outputs": [],
      "source": [
        "# Get the shape of the data array\n",
        "n_models, n_angles, image_size, _, _ = data.shape\n",
        "\n",
        "# Define training parameters\n",
        "batch_size = 18  # Number of samples in each training batch\n",
        "buffer_size = 1000 # Set the buffer size for shuffling\n",
        "steps_per_epoch = (n_models * n_angles * n_angles) // batch_size  # Number of steps per training epoch\n",
        "num_iterations = 1000 # Number of training epochs\n",
        "\n",
        "# - `n_models`: The number of different models in the dataset.\n",
        "# - `n_angles`: The number of angles for each model.\n",
        "# - `image_size`: The size of the images (assuming square images).\n",
        "# - `batch_size`: The number of samples in each training batch. Adjust based on memory constraints.\n",
        "# - `steps_per_epoch`: The number of steps to complete one training epoch. It's calculated by dividing the total number of possible combinations by the batch size.\n",
        "# - `epochs`: The number of training epochs, i.e., how many times the entire dataset is used for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O6JZKpqsC_w2"
      },
      "outputs": [],
      "source": [
        "output_folder = '/content/drive/MyDrive/NVS_GAN/V1'\n",
        "os.makedirs(output_folder, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Ee7wg_gBwtg"
      },
      "outputs": [],
      "source": [
        "# Specify the path to the save training logs\n",
        "log_dir = os.path.join(output_folder, 'logs')\n",
        "# Specify the path to the saved model\n",
        "generator_model_path = os.path.join(output_folder, 'generator_model')\n",
        "# Specify the path to the save images\n",
        "image_path = os.path.join(output_folder, 'image_test')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c8NfnoBhMY4k"
      },
      "outputs": [],
      "source": [
        "# Create one-hot encoded angles\n",
        "one_hot_angles = tf.one_hot(np.arange(n_angles), depth=n_angles, dtype=tf.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zlyXoP3UMe1T"
      },
      "outputs": [],
      "source": [
        "# Define generator function\n",
        "def data_generator():\n",
        "    \"\"\"\n",
        "    A data generator function that yields training data batches.\n",
        "\n",
        "    Yields:\n",
        "        tuple: A tuple containing source image, source angle one-hot encoding, target image, and target angle one-hot encoding.\n",
        "    \"\"\"\n",
        "    while True:\n",
        "        for model_idx in range(n_models):\n",
        "            for source_angle_idx in range(n_angles):\n",
        "                # Extract the source angle one-hot encoding for the current source angle index\n",
        "                source_angle_one_hot = one_hot_angles[source_angle_idx]\n",
        "\n",
        "                # Extract the source image for the current chair model and source angle\n",
        "                source_image = data[model_idx, source_angle_idx]\n",
        "\n",
        "                for target_angle_idx in range(n_angles):\n",
        "                    # Extract the target image for the current chair model and target angle\n",
        "                    target_image = data[model_idx, target_angle_idx]\n",
        "\n",
        "                    # Extract the target angle one-hot encoding for the current target angle index\n",
        "                    target_angle_one_hot = one_hot_angles[target_angle_idx]\n",
        "\n",
        "                    # Extract the transformation angle one-hot encoding\n",
        "                    transformation_azimuth = np.remainder(target_angle_idx - source_angle_idx + n_angles, n_angles)\n",
        "                    transformation_one_hot = one_hot_angles[transformation_azimuth]\n",
        "\n",
        "                    # Yield a tuple containing source image, source angle one-hot encoding,\n",
        "                    # target image, and target angle one-hot encoding\n",
        "                    yield source_image, target_image, transformation_one_hot, source_angle_one_hot, target_angle_one_hot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K9r3QhPIMr70"
      },
      "outputs": [],
      "source": [
        "# Define the output types and shapes for the generator function\n",
        "output_types = (tf.float32, tf.float32, tf.float32, tf.float32, tf.float32)\n",
        "output_shapes = (\n",
        "    tf.TensorShape((image_size, image_size, 3)),\n",
        "    tf.TensorShape((image_size, image_size, 3)),\n",
        "    tf.TensorShape((n_angles,)),\n",
        "    tf.TensorShape((n_angles,)),\n",
        "    tf.TensorShape((n_angles,))\n",
        ")\n",
        "\n",
        "# Create the dataset using the defined output types and shapes\n",
        "dataset = tf.data.Dataset.from_generator(data_generator, output_types=output_types, output_shapes=output_shapes)\n",
        "\n",
        "# Batch the dataset\n",
        "dataset = dataset.batch(batch_size)\n",
        "\n",
        "# Shuffle the dataset\n",
        "dataset = dataset.shuffle(buffer_size=buffer_size)\n",
        "\n",
        "dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cbjP7uiNPMPX"
      },
      "outputs": [],
      "source": [
        "# Function to convert one-hot encoding to angle\n",
        "def one_hot_to_angle(one_hot_encoding):\n",
        "    \"\"\"\n",
        "    Converts a one-hot encoded array into its corresponding angle value.\n",
        "\n",
        "    Args:\n",
        "        one_hot_encoding (numpy.ndarray): One-hot encoded array with 1 at the index corresponding to the angle.\n",
        "\n",
        "    Returns:\n",
        "        float or None: The angle value in degrees if found, else None.\n",
        "    \"\"\"\n",
        "    # Find the indices where the one-hot encoding has a value of 1\n",
        "    angle_indices = np.where(one_hot_encoding == 1)[0]\n",
        "\n",
        "    # If no angle is found (all 0s in one-hot encoding), return None\n",
        "    if len(angle_indices) == 0:\n",
        "        return None\n",
        "\n",
        "    # Calculate the angle using the first angle index and the total number of angles\n",
        "    # The angle calculation formula: Angle = Index * (360 degrees / Total number of angles)\n",
        "    angle = angle_indices[0] * (360.0 / len(one_hot_encoding))\n",
        "    return angle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v73uyzE9RcC2"
      },
      "outputs": [],
      "source": [
        "def display_images_with_angles(source_image, target_image, source_angle_one_hot, target_angle_one_hot, predicted_image=None, test=False):\n",
        "    \"\"\"\n",
        "    Displays images alongside their associated angles.\n",
        "\n",
        "    Args:\n",
        "        source_image (numpy.ndarray): The image of the source angle.\n",
        "        target_image (numpy.ndarray): The image of the target angle.\n",
        "        source_angle_one_hot (numpy.ndarray): One-hot encoded source angle.\n",
        "        target_angle_one_hot (numpy.ndarray): One-hot encoded target angle.\n",
        "        predicted_image (numpy.ndarray, optional): An image predicted by a model (if available).\n",
        "        test (bool): Flag indicating whether to display predicted image (True) or not (False).\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    # Convert one-hot encoded angles to angle values\n",
        "    source_angle = one_hot_to_angle(source_angle_one_hot)\n",
        "    target_angle = one_hot_to_angle(target_angle_one_hot)\n",
        "\n",
        "    # Display images and angles\n",
        "    if test:\n",
        "        fig, axs = plt.subplots(1, 3, figsize=(12, 4))\n",
        "        axs[0].imshow(source_image)\n",
        "        axs[0].set_title(f'Source Image (Angle: {source_angle:.2f}°)')\n",
        "        axs[1].imshow(target_image)\n",
        "        axs[1].set_title(f'Target Image (Angle: {target_angle:.2f}°)')\n",
        "        axs[2].imshow(predicted_image)\n",
        "        axs[2].set_title(f'Predicted Image (Angle: {target_angle:.2f}°)')\n",
        "    else:\n",
        "        fig, axs = plt.subplots(1, 2, figsize=(8, 4))\n",
        "        axs[0].imshow(source_image)\n",
        "        axs[0].set_title(f'Source Image (Angle: {source_angle:.2f}°)')\n",
        "        axs[1].imshow(target_image)\n",
        "        axs[1].set_title(f'Target Image (Angle: {target_angle:.2f}°)')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zm5nOD894Q6v"
      },
      "outputs": [],
      "source": [
        "def save_images(source_image, target_image, predicted_image, source_encoding, target_encoding, save_dir):\n",
        "    \"\"\"\n",
        "    Save source, target, and predicted images with angle information and encodings in PNG format.\n",
        "\n",
        "    Args:\n",
        "        source_image (np.ndarray): Source image to be saved.\n",
        "        target_image (np.ndarray): Target image to be saved.\n",
        "        predicted_image (np.ndarray): Predicted image to be saved.\n",
        "        source_encoding (np.ndarray): One-hot encoding for the source angle.\n",
        "        target_encoding (np.ndarray): One-hot encoding for the target angle.\n",
        "        save_dir (str): Directory to save the images.\n",
        "    \"\"\"\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    # Convert one-hot encoded angles to angle values\n",
        "    source_angle = one_hot_to_angle(source_encoding)\n",
        "    target_angle = one_hot_to_angle(target_encoding)\n",
        "\n",
        "    source_filename = f\"source_{source_angle:.2f}.png\"\n",
        "    target_filename = f\"target_{target_angle:.2f}.png\"\n",
        "    predicted_filename = f\"predicted_{target_angle:.2f}.png\"\n",
        "\n",
        "    Image.fromarray(np.uint8(source_image * 255)).save(os.path.join(save_dir, source_filename))\n",
        "    Image.fromarray(np.uint8(target_image * 255)).save(os.path.join(save_dir, target_filename))\n",
        "    Image.fromarray(np.uint8(predicted_image * 255)).save(os.path.join(save_dir, predicted_filename))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6q3E8Qe96Su_"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "save_dir = os.path.join(output_folder, 'sample')\n",
        "\n",
        "i = random.randint(0, (batch_size))\n",
        "\n",
        "# Fetch and display a batch of data from the dataset\n",
        "source_images, target_images, transformation_one_hot, source_one_hot_angles, target_one_hot_angles = next(iter(dataset))\n",
        "\n",
        "display_images_with_angles(source_images[i], target_images[i], source_one_hot_angles[i], target_one_hot_angles[i])\n",
        "#save_images(source_images[i], target_images[i], target_images[i], source_one_hot_angles[i], target_one_hot_angles[i], save_dir)\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7SLJplNnD4mn"
      },
      "outputs": [],
      "source": [
        "# Define a custom layer for bilinear sampling\n",
        "class BilinearSamplingLayer(Layer):\n",
        "    def __init__(self, image_size, **kwargs):\n",
        "        # Initialize the layer with the specified image_size\n",
        "        self.image_size = image_size\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "    def call(self, tensors):\n",
        "        # Unpack the input tensors: original image and predicted flow\n",
        "        original_image, predicted_flow = tensors\n",
        "\n",
        "        # Apply dense image warp with predicted flow scaled by image size\n",
        "        warped_image = tfa.image.dense_image_warp(original_image, predicted_flow * self.image_size)\n",
        "        return warped_image\n",
        "\n",
        "    def compute_output_shape(self, tensor):\n",
        "        # Calculate the output shape based on the input tensor shape\n",
        "        input_shape = tensor[0]\n",
        "        return None, input_shape[1], input_shape[2], input_shape[3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sempIM_ULW1m"
      },
      "outputs": [],
      "source": [
        "def get_modified_decoder_layer(x_d0, x_e, current_attention_strategy, current_image_size, pred_flow=None):\n",
        "    # Skip connection Strategies\n",
        "    # (1) U-Net\n",
        "    if current_attention_strategy == 'u_net':\n",
        "        x_d = Concatenate()([x_e, x_d0])\n",
        "        x_e_rearranged = x_e\n",
        "    # (0) Vanilla\n",
        "    else:\n",
        "        x_d = x_d0\n",
        "        x_e_rearranged = None\n",
        "\n",
        "    return x_e_rearranged, x_d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4rNPY_C7xgwD"
      },
      "outputs": [],
      "source": [
        "pixel_normalizer = lambda x: (x - 0.5) * 2\n",
        "pixel_normalizer_reverse = lambda x: x / 2 + 0.5\n",
        "decoder_original_features = {}\n",
        "encoder_original_features = {}\n",
        "decoder_rearranged_features = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZVQe4nMeNYhZ"
      },
      "outputs": [],
      "source": [
        "def movnetv1():\n",
        "        # Build Keras model. Tried to follow the original paper as much as possible.\n",
        "        activation = 'relu'\n",
        "        current_image_size = image_size\n",
        "        image_input = Input(shape=(current_image_size, current_image_size, 3), name='image_input')\n",
        "        image_input_normalized = Lambda(pixel_normalizer)(image_input)\n",
        "\n",
        "        i = 0\n",
        "        x = image_input_normalized #image_input_normalized\n",
        "        x = Conv2D(8, kernel_size=(3, 3), strides=(2, 2), padding='same')(x)\n",
        "        x = ReLU()(x)\n",
        "        x = SeparableConv2D(16, kernel_size=(3, 3), strides=(1, 1), padding='same')(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = ReLU()(x)\n",
        "        i = 1\n",
        "        current_image_size = int(image_size / 2)\n",
        "        encoder_original_features[current_image_size] = x\n",
        "\n",
        "        while current_image_size > 2 :\n",
        "          x = SeparableConv2D(16 * (2 ** i), kernel_size=(3, 3), strides=(2, 2), padding='same')(x)\n",
        "          x = BatchNormalization()(x)\n",
        "          x = ReLU()(x)\n",
        "          x = SeparableConv2D(16 * (2 ** i), kernel_size=(3, 3), strides=(1, 1), padding='same')(x)\n",
        "          x = BatchNormalization()(x)\n",
        "          x = ReLU()(x)\n",
        "          i = i+1\n",
        "          current_image_size = int(current_image_size / 2)\n",
        "          if(current_image_size == 8):\n",
        "            for repeat in range (4):\n",
        "              x = SeparableConv2D(256, kernel_size=(3, 3), strides=(1, 1), padding='same')(x)\n",
        "              x = BatchNormalization()(x)\n",
        "              x = ReLU()(x)\n",
        "              repeat = repeat+1\n",
        "          encoder_original_features[current_image_size] = x\n",
        "\n",
        "        x = Flatten()(x)\n",
        "        hidden_layer_size = int(4096 / 256 * image_size)\n",
        "        x = Dense(hidden_layer_size, activation=activation)(x)\n",
        "        #x = Dense(hidden_layer_size, activation=activation)(x)\n",
        "\n",
        "        viewpoint_input = Input(shape=(n_angles, ), name='viewpoint_input')\n",
        "\n",
        "        v = Dense(128, activation=activation)(viewpoint_input)\n",
        "        v = Dense(256, activation=activation)(v)\n",
        "\n",
        "        concatenated = concatenate([x, v])\n",
        "        concatenated = Dense(hidden_layer_size, activation=activation)(concatenated)\n",
        "        #concatenated = Dense(hidden_layer_size, activation=activation)(concatenated)\n",
        "\n",
        "        d = Reshape((2, 2, 1024))(concatenated)\n",
        "        #d = SeparableConv2D(1024, kernel_size=(3, 3), strides=(1, 1), padding='same')(d)\n",
        "        d = ReLU()(d)\n",
        "        while current_image_size < image_size / 2 :\n",
        "          current_image_size = current_image_size * 2\n",
        "          # attention strategy at this layer.\n",
        "          current_attention_strategy = 'unet'\n",
        "          d = Conv2DTranspose(4 * (2 ** i), kernel_size=(3, 3), strides=(2, 2), padding='same')(d)\n",
        "          d = ReLU()(d)\n",
        "          #d = SeparableConv2D(4 * (2 ** i), kernel_size=(3, 3), strides=(1, 1), padding='same')(d)\n",
        "          #d = ReLU()(d)\n",
        "          i = i-1\n",
        "\n",
        "          x_d0 = d\n",
        "          x_e = encoder_original_features[current_image_size]\n",
        "          x_e_rearranged, x_d = get_modified_decoder_layer(x_d0, x_e, current_attention_strategy, current_image_size)\n",
        "          decoder_original_features[current_image_size] = x_d0\n",
        "          decoder_rearranged_features[current_image_size] = x_e_rearranged\n",
        "          d = x_d\n",
        "\n",
        "        #d = SeparableConv2D(8, kernel_size=(3, 3), strides=(1, 1), padding='same')(d)\n",
        "        #d = ReLU()(d)\n",
        "        '''\n",
        "        d = Conv2DTranspose(16, kernel_size=(3, 3), strides=(1, 1), padding='same')(d)\n",
        "        d = ReLU()(d)\n",
        "        d = Conv2DTranspose(8, kernel_size=(3, 3), strides=(1, 1), padding='same')(d)\n",
        "        d = ReLU()(d)\n",
        "        '''\n",
        "        # final flow\n",
        "        pred_flow = Conv2DTranspose(2, kernel_size=(3, 3), strides=(2, 2), padding='same')(d)\n",
        "\n",
        "        # fetch pixels from original image\n",
        "        pred_image = BilinearSamplingLayer(image_size)([image_input, pred_flow])\n",
        "        return Model(inputs=[image_input, viewpoint_input], outputs=[pred_image])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZiUXj0bau-pQ"
      },
      "outputs": [],
      "source": [
        "generator = movnetv1()\n",
        "#generator.summary()\n",
        "tf.keras.utils.plot_model(generator, show_shapes=True, dpi=64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ll5vmk56yulJ"
      },
      "outputs": [],
      "source": [
        "loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YnIvxueWy0KT"
      },
      "outputs": [],
      "source": [
        "# Load Pre-trained VGG16 model for feature extraction\n",
        "vgg16 = tf.keras.applications.VGG16(weights='imagenet', include_top=False, input_shape=(image_size, image_size, 3))\n",
        "vgg16.trainable = False # Freeze the VGG16 model\n",
        "\n",
        "# Define the layers for feature extraction\n",
        "#selected_layers = [vgg16.get_layer('block3_conv3').output, vgg16.get_layer('block4_conv3').output]\n",
        "selected_layers = [layer.output for layer in vgg16.layers if 'conv' in layer.name]\n",
        "\n",
        "# Create a custom model for feature extraction\n",
        "feature_extractor = Model(inputs=vgg16.input, outputs=selected_layers)\n",
        "\n",
        "# Define a loss function (e.g., Mean Squared Error)\n",
        "mse_loss = MeanSquaredError()\n",
        "\n",
        "def generator_loss(disc_generated_output, gen_output, target):\n",
        "    gan_loss = loss_object(tf.ones_like(disc_generated_output), disc_generated_output)\n",
        "    # Define the weights for each loss component\n",
        "    gan_weight = 0.01\n",
        "    mae_weight = 1\n",
        "    ssim_weight = 1\n",
        "    perceptual_weight = 1\n",
        "\n",
        "    # MAE Loss\n",
        "    mae = tf.reduce_mean(tf.abs(target - gen_output))\n",
        "\n",
        "    # SSIM Loss\n",
        "    ssim = 1 - tf.image.ssim(target, gen_output, max_val=1.0)\n",
        "\n",
        "    # Perceptual Loss\n",
        "    real_features = feature_extractor(target)\n",
        "    generated_features = feature_extractor(gen_output)\n",
        "    perceptual_loss = 0.0\n",
        "    for real_feat, gen_feat in zip(real_features, generated_features):\n",
        "        perceptual_loss += mse_loss(real_feat, gen_feat)\n",
        "\n",
        "    # Combine the losses using the defined weights\n",
        "    total_loss = gan_weight * gan_loss + mae_weight * mae + ssim_weight * ssim + perceptual_weight * perceptual_loss\n",
        "\n",
        "    # Custom metrics\n",
        "    custom_metrics = {\n",
        "        'gan_loss': gan_loss,\n",
        "        'mae': mae,\n",
        "        'ssim': ssim,\n",
        "        'perceptual_loss': perceptual_loss\n",
        "    }\n",
        "\n",
        "    return total_loss, gan_loss, mae, ssim, perceptual_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u9Tdglvl0_9i"
      },
      "outputs": [],
      "source": [
        "def downsample(filters, size, apply_batchnorm=True):\n",
        "  initializer = tf.random_normal_initializer(0., 0.02)\n",
        "\n",
        "  result = tf.keras.Sequential()\n",
        "  result.add(\n",
        "      tf.keras.layers.Conv2D(filters, size, strides=2, padding='same',\n",
        "                             kernel_initializer=initializer, use_bias=False))\n",
        "\n",
        "  if apply_batchnorm:\n",
        "    result.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "  result.add(tf.keras.layers.LeakyReLU())\n",
        "\n",
        "  return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ll6aNeQx8b4v"
      },
      "outputs": [],
      "source": [
        "def Discriminator():\n",
        "  initializer = tf.random_normal_initializer(0., 0.02)\n",
        "\n",
        "  inp = tf.keras.layers.Input(shape=[256, 256, 3], name='input_image')\n",
        "  tar = tf.keras.layers.Input(shape=[256, 256, 3], name='target_image')\n",
        "\n",
        "  x = tf.keras.layers.concatenate([inp, tar])  # (batch_size, 256, 256, channels*2)\n",
        "\n",
        "  down1 = downsample(16, 4, False)(x)  # (batch_size, 128, 128, 16)\n",
        "  down2 = downsample(32, 4)(down1)  # (batch_size, 64, 64, 32)\n",
        "  down3 = downsample(64, 4)(down2)  # (batch_size, 32, 32, 64)\n",
        "  down4 = downsample(128, 4)(down3)  # (batch_size, 16, 16, 128)\n",
        "  down5 = downsample(256, 4)(down4)  # (batch_size, 8, 8, 256)\n",
        "  down6 = downsample(512, 4)(down5)  # (batch_size, 4, 4, 512)\n",
        "  flat = Flatten()(down6)\n",
        "  last = Dense(1, activation='relu')(flat)\n",
        "  return tf.keras.Model(inputs=[inp, tar], outputs=last)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d6Ke0Gwm1nQT"
      },
      "outputs": [],
      "source": [
        "discriminator = Discriminator()\n",
        "tf.keras.utils.plot_model(discriminator, show_shapes=True, dpi=64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "13_2Qrgi2Hht"
      },
      "outputs": [],
      "source": [
        "def discriminator_loss(disc_real_output, disc_generated_output):\n",
        "  real_loss = loss_object(tf.ones_like(disc_real_output), disc_real_output)\n",
        "\n",
        "  generated_loss = loss_object(tf.zeros_like(disc_generated_output), disc_generated_output)\n",
        "\n",
        "  total_disc_loss = real_loss + generated_loss\n",
        "\n",
        "  return total_disc_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YFPivmFH2Q2E"
      },
      "outputs": [],
      "source": [
        "generator_optimizer = tf.keras.optimizers.Adam(0.0002, beta_1=0.5)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(0.0002, beta_1=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_rtsE70VMReH"
      },
      "outputs": [],
      "source": [
        "# Create a SummaryWriter for TensorBoard\n",
        "summary_writer = tf.summary.create_file_writer(log_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Kuzae2P8Y8MO"
      },
      "outputs": [],
      "source": [
        "# Training loop\n",
        "for iteration in range(num_iterations):\n",
        "    source_images_batch, target_images_batch, transformation_batch, source_angle_batch, target_angle_batch = next(iter(dataset))\n",
        "\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "        gen_output = generator([source_images_batch, transformation_batch], training=True)\n",
        "        real_output = discriminator([source_images_batch, target_images_batch], training=True)\n",
        "        fake_output = discriminator([source_images_batch, gen_output], training=True)\n",
        "\n",
        "        gen_total_loss, gen_gan_loss, gen_mae_loss, gen_ssim_loss, gen_perceptual_loss = generator_loss(fake_output, gen_output, target_images_batch)\n",
        "        disc_loss = discriminator_loss(real_output, fake_output)\n",
        "\n",
        "    # Compute the average loss per batch\n",
        "    avg_gen_total_loss = tf.reduce_mean(gen_total_loss)\n",
        "    avg_gen_gan_loss = tf.reduce_mean(gen_gan_loss)\n",
        "    avg_gen_mae_loss = tf.reduce_mean(gen_mae_loss)\n",
        "    avg_gen_ssim_loss = tf.reduce_mean(gen_ssim_loss)\n",
        "    avg_gen_perceptual_loss = tf.reduce_mean(gen_perceptual_loss)\n",
        "    avg_disc_loss = tf.reduce_mean(disc_loss)\n",
        "\n",
        "    gradients_of_generator = gen_tape.gradient(gen_total_loss, generator.trainable_variables)\n",
        "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "\n",
        "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
        "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
        "\n",
        "    # Log the losses using summary_writer\n",
        "    with summary_writer.as_default():\n",
        "        tf.summary.scalar(\"Generator Total Loss\", avg_gen_total_loss, step=iteration+1)\n",
        "        tf.summary.scalar(\"Generator GAN Loss\", avg_gen_gan_loss, step=iteration+1)\n",
        "        tf.summary.scalar(\"Generator MAE Loss\", avg_gen_mae_loss, step=iteration+1)\n",
        "        tf.summary.scalar(\"Generator SSIM Loss\", avg_gen_ssim_loss, step=iteration+1)\n",
        "        tf.summary.scalar(\"Generator Perceptual  Loss\", avg_gen_perceptual_loss, step=iteration+1)\n",
        "        tf.summary.scalar(\"Discriminator Loss\", avg_disc_loss, step=iteration+1)\n",
        "\n",
        "    # Print the losses\n",
        "    print('============================================================')\n",
        "    print(f'Iteration {iteration + 1}')\n",
        "    print(f'Total Loss: {float(avg_gen_total_loss.numpy()):.4f}')\n",
        "    print(f'Generator Loss: {float(avg_gen_gan_loss.numpy()):.4f}')\n",
        "    print(f'MAE: {float(avg_gen_mae_loss.numpy()):.4f}')\n",
        "    print(f'SSIM: {float(avg_gen_ssim_loss.numpy()):.4f}')\n",
        "    print(f'Perceptual: {float(avg_gen_perceptual_loss.numpy()):.4f}')\n",
        "    print(f'Discriminator Loss: {float(avg_disc_loss.numpy()):.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "5mSomTyRE-Qk"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir /content/drive/MyDrive/NVS_GAN/V1/logs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "P31oRbalMUlR"
      },
      "outputs": [],
      "source": [
        "# Save the generator model in TensorFlow SavedModel format\n",
        "generator_model_path = os.path.join(output_folder, 'generator_model')\n",
        "tf.saved_model.save(generator, generator_model_path)\n",
        "\n",
        "# Close the summary_writer\n",
        "summary_writer.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "r5wA8TAzDWu8"
      },
      "outputs": [],
      "source": [
        "# Fetch and display a batch of data from the dataset\n",
        "source_images, target_images, transformation_one_hot, source_one_hot_angles, target_one_hot_angles = next(iter(dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "bVdAR6_fleoV"
      },
      "outputs": [],
      "source": [
        "# Load the generator model\n",
        "loaded_generator = tf.saved_model.load(generator_model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "WCPCQkq9DOPm"
      },
      "outputs": [],
      "source": [
        "pred_images = loaded_generator([source_images, transformation_one_hot])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "uDefiGTcDaGQ"
      },
      "outputs": [],
      "source": [
        "i = random.randint(0, (batch_size))\n",
        "print(i)\n",
        "display_images_with_angles(source_images[i], target_images[i], source_one_hot_angles[i], target_one_hot_angles[i], pred_images[i], test=True)\n",
        "save_images(source_images[i], target_images[i], pred_images[i], source_one_hot_angles[i], target_one_hot_angles[i], image_path)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}